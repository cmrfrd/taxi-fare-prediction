{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that takes in a dataframe, removes outliers, problably normalize and scale data.\n",
    "\n",
    "Functions are set up in such a way where we should be able to batch pre-process the data without reading in the entire data set at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.externals import joblib\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set types\n",
    "# https://gis.stackexchange.com/questions/8650/measuring-accuracy-of-latitude-and-longitude/8674#8674\n",
    "# Floats have 7 decimal digits of precision, doubles 15. You only need around 7 for lat and long \n",
    "types_dict = {\n",
    "        \"fare_amount\": np.float16,\n",
    "        \"pickup_longitude\": np.float32, \n",
    "        \"pickup_latitude\": np.float32, \n",
    "        \"dropoff_longitude\": np.float32, \n",
    "        \"dropoff_latitude\": np.float32, \n",
    "        \"passenger_count\": np.int8,\n",
    "#         \"pickup_datetime\": np.int8,\n",
    "#         \"pickup_datetime\": np.datetime64[ns],\n",
    "        \"pickup_year\": np.float32,\n",
    "        \"pickup_hour\": np.float32,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_outliers(df):\n",
    "    \"\"\"Takes in an unprocessed df and returns a df that has outliers removed\"\"\"\n",
    "    ## Remove obvious outliers first\n",
    "    # where lat/long == 0\n",
    "    df = df.drop(df.index[df['pickup_longitude'] == 0])\n",
    "    df = df.drop(df.index[df['dropoff_longitude'] == 0])\n",
    "    # Where fares < 0, becuase that makes no sense\n",
    "    df = df.drop(df.index[df['fare_amount'] < 0])\n",
    "    # Realistically, if we want to keep the trips within manhattan, pickup long should be between -72 and -75 and lat between 40 and 42\n",
    "    # as everything else is an extreme outlier\n",
    "    df = df.drop(df.index[df['pickup_longitude'] > -72])\n",
    "    df = df.drop(df.index[df['pickup_longitude'] < -75])\n",
    "    df = df.drop(df.index[df['pickup_latitude'] > 42])\n",
    "    df = df.drop(df.index[df['pickup_latitude'] < 40])\n",
    "    # Visualizations show the same thing with dropof longs and lats\n",
    "    df = df.drop(df.index[df['dropoff_longitude'] > -72])\n",
    "    df = df.drop(df.index[df['dropoff_longitude'] < -75])\n",
    "    df = df.drop(df.index[df['dropoff_latitude'] > 42])\n",
    "    df = df.drop(df.index[df['dropoff_latitude'] < 40])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we're assuming that a df of the right types is being passed in\n",
    "data = namedtuple('data', 'training_data fares_data')\n",
    "\n",
    "def preprocess(df,types=types_dict):\n",
    "    \"\"\"Takes in an unprocessed df and returns a tuple df that has outliers removed and is preprocessed\"\"\"\n",
    "    # Drop NA values\n",
    "    df = df.dropna()\n",
    "    # Remove obvious outliers first\n",
    "    df = _remove_outliers(df)\n",
    "    # Key isnt useful\n",
    "    del df[\"key\"]\n",
    "    # Currently giving categorial numbers, but something like onehotencoding might work better\n",
    "    # df[\"pickup_datetime\"] = df[\"pickup_datetime\"].apply(lambda d:int(d[10:13]))\n",
    "    date_format_string = \"%Y-%m-%d %H:%M:%S UTC\" \n",
    "    df[\"pickup_datetime\"] = pd.to_datetime(df[\"pickup_datetime\"].iloc[:10000000], format=date_format_string)\n",
    "    df[\"pickup_year\"] = df[\"pickup_datetime\"].apply(lambda t: t.year + t.dayofyear/365) \n",
    "    df[\"pickup_hour\"] = df[\"pickup_datetime\"].apply(lambda t: t.hour + t.minute/(60) + t.second/(60*60))\n",
    "    del df[\"pickup_datetime\"]\n",
    "    # Set types as we batch read in\n",
    "    for name, t in types_dict.items():\n",
    "        df[name] = df[name].astype(t)\n",
    "    fares = df[\"fare_amount\"]\n",
    "    del df[\"fare_amount\"]\n",
    "    # TODO - Scale data. Requires us to find min and max of entire dataset, not sure how to proceed since we're batching\n",
    "    # TODO - Normalize data, same issues as above and batching.\n",
    "    # TODO - Possibly PCA, but seems l'ike all of the features we get should be relevant to the task at hand\n",
    "    return data(training_data=df, fares_data=fares)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(df):\n",
    "    '''Takes in a df of similar format to the original dataset and normalizes using a pickled normalizer trained on the entire dataset'''\n",
    "    try:\n",
    "        n = joblib.load('../../models/normalizer.joblib')\n",
    "    except:\n",
    "        print(\"Cannot load normalizer model, does it exist?\")\n",
    "        return None\n",
    "    return pd.DataFrame(n.transform(df), columns=list(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale(df):\n",
    "    '''Takes in a df of similar format to the original dataset and scales using a pickled scaler trained on the entire dataset'''\n",
    "    try:\n",
    "        n = joblib.load('../../models/minmaxscaler.joblib')\n",
    "    except:\n",
    "        print(\"Cannot load scaler model, does it exist?\")\n",
    "        return None\n",
    "    return pd.DataFrame(n.transform(df), columns=list(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we add new features to our dataset, we will need to normalize and scale those as well. However, we can't just use our already pickled normalizer and scaler as those were trained on the original dataset and not the new columns we are trying to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_new_feature(series, filename, type_of):\n",
    "#     try:\n",
    "#         n = joblib.load('../../models/{}_normalizer.joblib'.format(series.name))\n",
    "#     except:\n",
    "#     scaler = MinMaxScaler().fit(series)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_new_feature(df):\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

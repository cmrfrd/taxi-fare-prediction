{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to generate a MinMaxScaler and Normalizer object on the entire data set, then pickle them for later use.\n",
    "\n",
    "This does not need to be run every time, just when we need to update the MinMaxScaler objects and Normalizer objects in the models folder because we've added a feature or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [09:22<00:00,  6.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entire dataset fits in 1.74 GB\n",
      "Number of rows: 52756015\n"
     ]
    }
   ],
   "source": [
    "### test functions\n",
    "chunksize = 539000 # rows in df / 100\n",
    "filename = \"../data/train.csv\"\n",
    "\n",
    "dataset_df = pd.DataFrame()  # main dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Pretty progress bar\n",
    "pbar = tqdm(total=len(range(100)))       \n",
    "for index, df_chunk in enumerate(pd.read_csv(filename, chunksize=chunksize)):\n",
    "    pbar.update(1)\n",
    "    dataset_df = pd.concat([dataset_df, preprocess(df_chunk).training_data])\n",
    "pbar.close()\n",
    "\n",
    "size_bytes = dataset_df.memory_usage(deep=True).sum()\n",
    "print(\"Entire dataset fits in {:.2f} GB\".format(size_bytes/1e+9))\n",
    "print(\"Number of rows: {}\".format(dataset_df.count()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/.conda/envs/jovyan/lib/python3.6/site-packages/sklearn/preprocessing/data.py:323: DataConversionWarning: Data with input dtype int8, float32 were all converted to float64 by MinMaxScaler.\n",
      "  return self.partial_fit(X, y)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler().fit(dataset_df)\n",
    "# then on new data call scaler.transform(data) to scale\n",
    "with open('../models/minmaxscaler.joblib', 'wb') as file:\n",
    "    joblib.dump(scaler, file) \n",
    "del scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We need to normalize on scaled data I think, but I cant fit the scaled df into memory along with the original df\n",
    "normalizer = Normalizer().fit(dataset_df)\n",
    "with open('../models/normalizer.joblib', 'wb') as file:\n",
    "    joblib.dump(normalizer, file) \n",
    "del normalizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook to generate a MinMaxScaler and Normalizer object on the entire data set, then pickle them for later use.\n",
    "\n",
    "This does not need to be run every time, just when we need to update the MinMaxScaler objects and Normalizer objects in the models folder because we've added a feature or something."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run Preprocessing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [02:29<00:00,  2.86s/it]"
     ]
    }
   ],
   "source": [
    "### test functions\n",
    "chunksize = 10 ** 6\n",
    "filename = \"../data/train.csv\"\n",
    "\n",
    "dataset_df = pd.DataFrame()  # main dataset\n",
    "\n",
    "from tqdm import tqdm\n",
    "# Pretty progress bar\n",
    "pbar = tqdm(total=len(range(54)))       \n",
    "for index, df_chunk in enumerate(pd.read_csv(filename, chunksize=chunksize)):\n",
    "    pbar.update(1)\n",
    "    dataset_df = pd.concat([dataset_df, preprocess(df_chunk).training_data])\n",
    "pbar.close()\n",
    "\n",
    "size_bytes = dataset_df.memory_usage(deep=True).sum()\n",
    "print(\"Entire dataset fits in {:.2f} GB\".format(size_bytes/1e+9))\n",
    "print(\"Number of rows: {}\".format(dataset_df.count()[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note may not overwrite old pickled files as we dump them, but append instead\n",
    "scaler = MinMaxScaler().fit(dataset_df)\n",
    "# then on new data call scaler.transform(data) to scale\n",
    "with open('../models/minmaxscaler.joblib', 'wb') as file:\n",
    "    joblib.dump(scaler, file) \n",
    "del scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  We need to normalize on scaled data I think, but I cant fit the scaled df into memory along with the original df\n",
    "normalizer = Normalizer().fit(dataset_df)\n",
    "with open('../models/normalizer.joblib', 'wb') as file:\n",
    "    joblib.dump(normalizer, file) \n",
    "del normalizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
